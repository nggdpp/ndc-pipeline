# ndc-pipeline
NDC collection processing pipeline

This project contains the basic processing logic for working over the National Digital Catalog, using collection metadata to determine processing configuration details, and then executing processes to put the data online for use. I'm still working out exactly what kind of architecture the workflows will be hung on, so for right now, these are just built as individual python scripts run locally or in a cloud virtual environment.

All of the processing logic is contained in functions of the pynggdpp package called from scripted workflows in this project. Since this code is building infrastructure online and writing data to database components, it relies on a set of environment variables specific to where the code is deployed. You'll see these set with a function from pynggdpp and calling an environment file. As such, the workflows will only work if you have access to that information, but I'm making them public anyway to allow for discussion and collaboration as we start discussing this work with State Geological Surveys and other stakeholders.

The process steps will be described in more detail within the docs package of the pynggdpp project. The following provides a running directory of what I'm committing here.

* ndc_collections.py - Reads items from ScienceBase classed as ndc_collections - the individual collections of geological or geophysical data organized into the catalog. These are read out and stashed in a MongoDB collection for later reference. This will serve as the versioning space for collections as they change and need to be evaluated for the impact of those changes.
* ndc_files.py - Works over all of the file attachments referenced from the collections to determine a bit more information about what they are and whether or not they represent a potential processing route to retrieve collection records. There is still lots of messiness in this end of the process as there is no consistent discipline provided in how files are cataloged and documented. My current process can be much improved once we clarify the file situation and institute some business rules on how files are provided. Data from this process is stashed into an ndc_files collection in a MongoDB database. This will also become a persistent store to handle file versioning and processing over time as we get that worked out.
* ndc_weblinks.py - Works over all of the web links shared by collection items. This is mostly just looking for advertised "Web Accessible Folders" at this point, where it verifies that they can be accessed and stashes a list of URLs to harvestable XML files. I'll be experimenting with how distribution links from the collection metadata can point to other types of harvestable end points pulled into the indexing process.
* file_processor.py - Works over the ndc_files collection to process everything with a file processing route. The basic notion at this point is to standardize syntactically to GeoJSON structures for every collection file. That builds at least a standards compliant geometry for every record, including blank coordinates (null geometry) when no coordinates are provided or coordinates can't be processed into valid geometry. There will be a variable properties collection for each record, containing whatever was pulled out of the records. The file_meta() function run from the ndc_files process builds the start to a property registry for each collection file, and these will be used to start working through semantic integration routines as we work to pull more intelligence out of the current information, and more importantly, work on better collection information sources from providers. This process builds out individual GeoJSON feature collections for each collection and then puts those features into their own MongoDB collections that are later piped into ElasticSearch and optimized for use.
 