# ndc-pipeline
NDC collection processing pipeline

This project contains the basic processing logic for working over the National Digital Catalog, using collection metadata to determine processing configuration details, and then executing processes to put the data online for use. I ended up developing two completely different workflows for processing NDC collections based. One is a serverless approach (in the serverless folder) using SQS messaging, AWS Lambda for processing, and feeding data directly into Elasticsearch. The other approach, dubbed "serverful," builds data into a MongoDB instance first. I plan to ultimately implement the serverless approach as it operates much faster overall with a minimum of resource usage and information redundancy, but we have to first work out how to deploy that architecture on USGS cloud resources. 

All of the processing logic is contained in functions of the pynggdpp package called from scripted workflows in this project. Since this code is building infrastructure online and writing data to database components, it relies on a set of environment variables specific to where the code is deployed. As such, the workflows will only work if you have access to that information or you set up a local environment of your choosing. I built the initial serverless approach using the [localstack project](https://github.com/localstack/localstack).

Each approach essentially does the same thing with a little bit of adaptation to the environment where data are flowing. It starts with reading all of the items in ScienceBase that are flagged as NDC collections. In the serverless approach, pertinent ScienceBase Item documents are put onto an SQS queue. In the serverful approach, they are built into a MongoDB collection. In the messaging approach, a next step works over the collections to decide what needs to be done and then builds out new messages for file processing and generates or updates indexed collection records. In the serverful approach, since the store of collection information persists, we go back to it a couple of times for further processing.

The collection metadata processing phase of the workflow assembles some standardized properties for every collection, essentially the small bit of identification information that we want to include in the master item index to aid in discovery and access. In both approaches, collection information is built into either an Elasticsearch index or MongoDB collection used to drive API functionality and to infuse meta properties into individual item records. In the serverless approach, I built collection introspection notes about possible improvements into the records. In the serverful approach, I rely on subsequent queries to expose things like missing or incomplete information. 

The next phase of processing works through the two different methods for collections to present their items - files stored in the ScienceBase repository or links to "web accessible folders." Either case results in a set of files to be harvested from URLs, digested into their individual records, and loaded into an index or collection for online access through the API. I create a "uniform file object" for each route to collection data that contains the necessary information about how to access the file and all the meta infusion properties. These file documents are either sent to a message queue for processing or stored in a MongoDB collection.

File processing was really the trickiest part of working out this architecture. As typical, a whole lot of messiness was introduced with text files in terms of encoding issues and "less than optimal" data structures. In the current instantiation of a "sort of" search index in ScienceBase, a lot of manual work has been done to clean up messy files and process them into "child items" using a background ScienceBase tool for digesting files into items. I ended up relying on a flag set in the ScienceBase file objects when this process was kicked off that I'm not entirely confident in. It indicates that a given attached file has been processed, presumably successfully. Filtering the "actionable files" based on this value helped to narrow down the cruft, but it may have left off some legitimate source files that should be processed. As part of cleanup work, we need to go through the collections and deliberately flag the files that should be used, and either remove or otherwise distinguish files that are associated with the collection in some way but do not represent source material for the inventory.

After experimenting with an approach in the serverless architecture that first pulled all files for processing into an S3 bucket, I ended up going back to real-time processing from source URLs (ScienceBase or some other server via WAF). (There are still some functions in the serverless folder that ran things according to an S3 cache.) The S3 approach could provide some advantages in that it breaks the process up into smaller chunks, makes the final read and process step on the server much faster (for the serverful approach), integrates nicely with Lambda and other AWS tools we might experiment with (e.g., Athena), and could allow us to implement file versioning abstracted from whatever source repositories are doing. However, it introduced a fair bit of overhead into my overall processing pipeline that I found to be distracting. In the long run, we probably need a solution that respects when a source repository advertises that its files are already on S3 (or maybe other cloud stores) and is smart enough to send its processing to the data rather than shipping bytes around at all. In the near term, all of these files are small enough that reading them into memory isn't that big a deal.

Three core functions in the pynggdpp package handle file processing. They are uglier than I'd like, and I'll have to get back into refining them at some point. Parsing XML sources, both "NGGDPP XML" sets of records and WAF-harvested formal metadata (FGDC CSDGM and ISO19139), was relatively straightforward and little prone to errors. I used the [gis_metadata_parser ](https://github.com/consbio/gis-metadata-parser) project as a convenient way of synthesizing abstract metadata elements from CSDGM and ISO metadata. I used the [xmltodict](https://github.com/martinblech/xmltodict) package as a convenience in dealing with the simple NGGDPP XML format. After experimenting with about a half dozen different ways of working over the various flavors of text files, I ended up using Pandas. This puts a fair bit of overhead on the project dependencies, particularly when deploying functions to Lambda, but it wound up being the least fraught with corner case problems to result in a reasonably competent recordset for processing.

The end result of each of the core file processing functions is a recordset in a list of dictionaries. Each record (only one in the case of an individual WAF-listed metadata document) will contain a potentially variable set of source properties along with infused file and collection metadata properties. The record build process does a little bit of processing on supplied coordinates (or bounding boxes in the case of formal metadata) to try and build a legitimate GeoJSON point object and a geopoint form of the coordinates for spatial indexing and searches. All of the infused or generated properties use the "ndc_" prefix for a little bit of separation from source properties. The record build process also includes a minimal amount of processing on the two different date fields with an attempt to make them into legitimate date type fields. This is particularly important with Elasticsearch indexing as it is quite picky about date formatting.

Spatial and temporal homogenization and synthesis are really the only two parts of the data model I've messed with at all at this point and those only minimally so. In the long run, we need to do more with those attributes and with other properties in the data such as type classification on physical artifact descriptions, but I opted to hold off for now as we explore other ways of getting access to potentially much richer information sources from providers. In future architecture, spatial, temporal, and other processing need to be split out into post-processing microservice-generated annotations as opposed to being built into the initial processing pipeline.

In both architectural approaches, each individual collection gets its own index (Elasticsearch) or collection (MongoDB). While both document database infrastructures would approach building one large collection for everything even with variable schemas, I found it easier to simply throw them each into their own logical collection space. In cases where a collection presents with multiple files for processing, all source files do end up in the same index/collection named with the collection identifier from ScienceBase. Each index/collection will then have some of the same properties that we can count on in various search patterns and some variable properties that will require more work to understand and synthesize for common use.

In the current working set of infrastructure we are running on the USGS cloud, we do have a pattern for storing data in a persistent store like MongoDB or PostgreSQL and then pushing data into Elasticsearch. For the first iteration of actually putting this system into production, the path of least resistance if probably the serverful approach. We have a design pattern for packing the scripts and dependencies via Docker, deploying to an EC2 instance via GitLab and Jenkins, running a composition of processes, and spinning down the machine when finished. We have an API that handles the operational process of pushing MongoDB collections to Elasticsearch indexes to get around security restrictions on how production data are built. In the longer term, I don't see any particular reason to put MongoDB in the middle rather than simply pushing data directly to Elasticsearch from source files. There are some conveniences to having in-process data in MongoDB collections as opposed to SQS queues in terms of running aggregations that put more of the workload on the database, but then we're storing the data twice and running a whole other set of mostly duplicative infrastructure. We can eventually handle record-level versioning, something I didn't deal with yet, in either case, and both document stores lend themselves to microservice-generated annotations.